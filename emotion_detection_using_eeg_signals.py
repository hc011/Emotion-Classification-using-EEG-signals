# -*- coding: utf-8 -*-
"""Emotion detection using EEG signals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uxln-84sojCkR4xaEVDzNQenYxlFaYDf
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

import tensorflow as tf

from sklearn.metrics import confusion_matrix, classification_report

df = pd.read_csv('/content/drive/MyDrive/minor project 2/dataset/emotions.csv')

sample = df.loc[0, 'fft_0_b':'fft_749_b']

plt.figure(figsize=(16, 10))
plt.plot(range(len(sample)), sample)
plt.title("Features fft_0_b through fft_749_b")
plt.show()

label_mapping = {'NEGATIVE': 0, 'NEUTRAL': 1, 'POSITIVE': 2}

"""# Data Splitting"""

def preprocess_inputs(df):
    df = df.copy()
    
    df['label'] = df['label'].replace(label_mapping)
    
    y = df['label'].copy()
    X = df.drop('label', axis=1).copy()
    
    x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)
    
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = preprocess_inputs(df)

"""#Model Creation

"""

inputs = tf.keras.Input(shape=(x_train.shape[1],))

expand_dims = tf.expand_dims(inputs, axis=2)

lstm = tf.keras.layers.LSTM(256, return_sequences=True)(expand_dims)

flatten = tf.keras.layers.Flatten()(lstm)

outputs = tf.keras.layers.Dense(3, activation='softmax')(flatten)


model = tf.keras.Model(inputs=inputs, outputs=outputs)
print(model.summary())

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history_lstm = model.fit(
    x_train,
    y_train,
    validation_split=0.2,
    batch_size=32,
    epochs=50,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True
        )
    ]
)

plt.plot(history_lstm.history['accuracy'])
plt.plot(history_lstm.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history_lstm.history['loss'])
plt.plot(history_lstm.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

model_acc = history_lstm.model.evaluate(x_test, y_test, verbose=0)[1]
print("Test Accuracy: {:.3f}%".format(model_acc * 100))

y_pred=np.array(list(map(lambda x: np.argmax(x), model.predict(x_test))))

cm=confusion_matrix(y_test,y_pred)
clr=classification_report(y_test,y_pred,target_names=label_mapping.keys())

plt.figure(figsize=(8,8))
sns.heatmap(cm,annot=True, vmin=0, fmt='g',cbar=False,cmap='Blues')
plt.xticks(np.arange(3)+0.5,label_mapping.keys())
plt.yticks(np.arange(3)+0.5,label_mapping.keys())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print(clr)



print(y_test)
model.predict(x_test)

"""#RNN implementation"""

print("x_train and y_train")
print(x_train.shape,y_train.shape)
print("\n---------------------\n")
print("x_test and y_test")
print(x_test.shape,y_test.shape)

inputs = tf.keras.Input(shape=(x_train.shape[1],))

x=tf.keras.layers.Dense(64, activation='relu')(inputs)
x=tf.keras.layers.Dense(64, activation='relu')(x)

outputs = tf.keras.layers.Dense(3, activation='softmax')(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
print(model.summary())

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history_rnn = model.fit(
    x_train,
    y_train,
    validation_split=0.2,
    batch_size=32,
    epochs=50,
)

model_acc = history_rnn.model.evaluate(x_test, y_test, verbose=0)[1]
print("Test Accuracy: {:.3f}%".format(model_acc * 100))

plt.plot(history_rnn.history['accuracy'])
plt.plot(history_rnn.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history_rnn.history['loss'])
plt.plot(history_rnn.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

y_pred=np.array(list(map(lambda x: np.argmax(x), model.predict(x_test))))

cm=confusion_matrix(y_test,y_pred)
clr=classification_report(y_test,y_pred,target_names=label_mapping.keys())

plt.figure(figsize=(8,8))
sns.heatmap(cm,annot=True, vmin=0, fmt='g',cbar=False,cmap='Blues')
plt.xticks(np.arange(3)+0.5,label_mapping.keys())
plt.yticks(np.arange(3)+0.5,label_mapping.keys())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print(clr)

"""#GRU implementation"""

inputs = tf.keras.Input(shape=(x_train.shape[1],))

expand_dims = tf.expand_dims(inputs, axis=2)

gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)

flatten = tf.keras.layers.Flatten()(gru)

outputs = tf.keras.layers.Dense(3, activation='softmax')(flatten)


model = tf.keras.Model(inputs=inputs, outputs=outputs)
print(model.summary())

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history2 = model.fit(
    x_train,
    y_train,
    validation_split=0.2,
    batch_size=32,
    epochs=50,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True
        )
    ]
)

model_acc = history2.model.evaluate(x_test, y_test, verbose=0)[1]
print("Test Accuracy: {:.3f}%".format(model_acc * 100))

plt.plot(history2.history['accuracy'])
plt.plot(history2.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print(y_test)
model.predict(x_test)

y_pred=np.array(list(map(lambda x: np.argmax(x), model.predict(x_test))))

cm=confusion_matrix(y_test,y_pred)
clr=classification_report(y_test,y_pred,target_names=label_mapping.keys())

plt.figure(figsize=(8,8))
sns.heatmap(cm,annot=True, vmin=0, fmt='g',cbar=False,cmap='Blues')
plt.xticks(np.arange(3)+0.5,label_mapping.keys())
plt.yticks(np.arange(3)+0.5,label_mapping.keys())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print(clr)